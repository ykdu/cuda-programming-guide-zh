# 1.1 Introduction

### 1.1.1. 图形处理单元（GPU）的诞生

 GPU最初是作为一种专用处理器用于3D图形处理，它起初是作为固定功能硬件来加速实时3D渲染中的并行操作。随着GPU的不断发展，逐渐具备了更多的可编程性。到了2003年，图形管线的某些阶段已经完全可编程，能够并行运行自定义代码来处理3D场景或图像的每个组件。

2006年，NVIDIA推出了计算统一设备架构（CUDA），使得任何计算工作负载都可以独立于图形API使用GPU的吞吐能力。

此后，CUDA和GPU计算被用于加速几乎所有类型的计算工作负载，从科学模拟（如流体动力学或能量传输）到商业应用（如数据库和分析）。此外，GPU的能力和可编程性为新算法和技术的进步奠定了基础，这些技术包括从图像分类到生成性人工智能（如扩散模型或大语言模型）等。

### 1.1.2 使用GPU的好处

 在相似的价格和功率范围内，GPU提供的指令吞吐量和内存带宽远高于CPU。许多应用程序利用这些能力在GPU上运行时比在CPU上要快得多（参见GPU应用）。其他计算设备，如FPGA，也具有很高的能效，但其编程灵活性远不及GPU。

GPU和CPU的设计目标不同。CPU的设计目标是尽可能快速地执行一系列串行操作（称为线程），并且能够并行执行几十个线程，而GPU的设计目标是擅长并行执行成千上万的线程，通过牺牲较低的单线程性能来实现更高的总吞吐量。

GPU专门用于高度并行的计算，并将更多的晶体管分配给数据处理单元，而CPU则将更多的晶体管分配给数据缓存和流控制。图1展示了CPU和GPU的芯片资源分配示例。

![](https://docs.nvidia.com/cuda/cuda-programming-guide/_images/gpu-devotes-more-transistors-to-data-processing.png)

*Figure 1：GPU将更多的晶体管用于数据处理*

### 1.1.3 快速入门

 有多种方式可以利用GPU提供的计算能力。本指南介绍了如何在高层语言（如C++）中为CUDA GPU平台编程。然而，也有许多方法可以在不直接编写GPU代码的应用程序中利用GPU。

有越来越多的算法和常规库可供使用，涵盖了多个领域。当某个库已经实现，特别是NVIDIA提供的库时，使用它往往比从头重新实现算法更高效、更具性能。例如，cuBLAS、cuFFT、cuDNN和CUTLASS等库就是一些帮助开发者避免重新实现已经成熟算法的例子。这些库的额外好处是，它们针对每个GPU架构进行了优化，提供了生产力、性能和可移植性的理想组合。

还有一些框架，特别是用于人工智能的框架，提供了GPU加速的构建块。这些框架中的许多通过利用上述GPU加速的库来实现加速。

此外，像NVIDIA的Warp或OpenAI的Triton这样的领域特定语言（DSL）可以编译直接在CUDA平台上运行。这为GPU编程提供了一种比本指南中涵盖的高层语言更高级的方法。

[NVIDIA加速计算中心](https://github.com/NVIDIA/accelerated-computing-hub)包含了资源、示例和教程，帮助学习GPU和CUDA计算。
